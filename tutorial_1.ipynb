{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to keras\n",
    "\n",
    "This is introductory kernel for ARC members to get used to keras interface. The tutorial will first begin by developing a simple Multi-Layer Perceptron (MLP) model.\n",
    "\n",
    "MLP: https://en.wikipedia.org/wiki/Multilayer_perceptron\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from keras import layers\n",
    "from keras.optimizers import *\n",
    "from keras.models import Model\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n,d,sigma):\n",
    "    x = np.random.randn(d,n)\n",
    "    w = []\n",
    "    for i in range(1,d + 1):\n",
    "        w.append(i / k)\n",
    "        \n",
    "    #w = np.expand_dims(np.array(w), axis = -1)\n",
    "    y = np.matmul(w,x) + sigma * np.random.randn(n)\n",
    "    \n",
    "    return x, y, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first start by generating a synthetic data for this tutorial. For the next tutorial, we will be building a CNN based on 'real' data using the MNIST dataset. You are welcome to try different values for the data and experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 500 #number of samples\n",
    "k = 10 #number of features\n",
    "s = 1 #standard deviation\n",
    "\n",
    "x_train, y_train, w_true = generate_data(n,k,s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 500)\n"
     ]
    }
   ],
   "source": [
    "#.shape returns the shape of your numpy array. It will come in handy.\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we split our data into training and validation set, we won't handle test set in this specific tutorial. Just run the code to split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(x,y):\n",
    "    n = x.shape[1]\n",
    "    indexes = np.random.permutation(n)\n",
    "    cutoff = int(n * 0.8)\n",
    "    TRAIN = indexes[0: cutoff]\n",
    "    VAL = indexes[cutoff::]\n",
    "\n",
    "    train_data = h(x[TRAIN, :], b, G)\n",
    "    train_label = y[TRAIN]\n",
    "    val = h(x[VAL, :], b, G)\n",
    "    val_label = y[VAL]\n",
    "    return train_data, train_label, val, val_label\n",
    "    \n",
    "train_x, train_y, val_x,val_y = split(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your code starts from here!\n",
    "From this point onwards, you will have to complete parts of the code to run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# builds a MLP model\n",
    "def build_model():\n",
    "    K.clear_session()\n",
    "    #=====YOUR CODE HERE=========\n",
    "    \n",
    "    #define input layer\n",
    "    #link a dense layer of 32 filters to input, activation relu\n",
    "    #link a dense layer of 64, activation relu\n",
    "    #link a Dropout layer (rate=0.5)\n",
    "    #link a dense layer of 16, activation relu\n",
    "    #link a dense layer of 1, with no activation\n",
    "    #define Model with input and output\n",
    "    #=====END OF CODE============\n",
    "    return model\n",
    "\n",
    "model = model()\n",
    "#summary shows you the entire model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With your model finished, you need to compile your model with respective loss function and an optimizer for it to perform gradient descent. Also, we can define metric that will give us good performance value to look at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#For each function, define batchsize=32 and verbose=1. Define epoch as to whatever value you want.\n",
    "#For now, consider more epochs --> better loss, but this is not necessarily true. You can figure this out\n",
    "#when you evaluate models trained with different epochs. e.g. 10 vs 200 vs 1000.\n",
    "\n",
    "#=====YOUR CODE HERE=========\n",
    "#compile model with adam optimizer, mse loss, and accuracy metric\n",
    "#fit(train) your model to the train data and train label, use your validation data by including it in your argument\n",
    "#evaluate your model on the validation dataset\n",
    "#=====END OF CODE============"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congrats!\n",
    "\n",
    "You just built your own neural network! You have already familiarized yourself with keras. Next time we will build something that pertains more to our actual project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
